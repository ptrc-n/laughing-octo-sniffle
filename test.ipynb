{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "import silence_tensorflow.auto\n",
    "import numpy as np\n",
    "\n",
    "N_TIMESTEPS = 5\n",
    "MAX_N_HARPS = 5\n",
    "N_FEATURES = 21\n",
    "INPUT_DIM = (N_TIMESTEPS, MAX_N_HARPS, N_FEATURES)\n",
    "N_OUT = 2\n",
    "OUTPUT_DIM =(N_TIMESTEPS,N_OUT)\n",
    "NUM_LAYERS = 3\n",
    "D_MODEL = 12\n",
    "NUM_HEADS = 3\n",
    "\n",
    "assert N_FEATURES > D_MODEL\n",
    "assert D_MODEL % NUM_HEADS == 0\n",
    "\n",
    "t = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=24,\n",
    "    input_dimensions=INPUT_DIM,\n",
    "    target_dimensions=OUTPUT_DIM,\n",
    "    rate=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.random.random(INPUT_DIM) for _ in range(10)])\n",
    "y = np.array([np.random.random(OUTPUT_DIM) for _ in range(10)])\n",
    "\n",
    "y_pred, _ = t((X, y), training=True)\n",
    "np.any(np.isnan(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "sharp_df = pd.read_csv('data/sharp.csv')\n",
    "sharp_columns = [\n",
    "    \"timestamp\",\n",
    "    \"harp\",\n",
    "    \"USFLUX\",\n",
    "    \"MEANGAM\",\n",
    "    \"MEANGBT\",\n",
    "    \"MEANGBZ\",\n",
    "    \"MEANGBH\",\n",
    "    \"TOTPOT\",\n",
    "    \"TOTUSJZ\",\n",
    "    \"TOTUSJH\",\n",
    "    \"ABSNJZH\",\n",
    "    \"SAVNCPP\",\n",
    "    \"MEANPOT\",\n",
    "    \"MEANSHR\",\n",
    "    \"SHRGT45\",\n",
    "    \"SIZE\",\n",
    "    \"SIZE_ACR\",\n",
    "    \"NACR\",\n",
    "    \"NPIX\",\n",
    "    \"MEANJZD\",\n",
    "    \"MEANALP\",\n",
    "    \"MEANJZH\",\n",
    "]\n",
    "\n",
    "sharp_df = sharp_df[sharp_columns]\n",
    "sharp_df = sharp_df.dropna()\n",
    "\n",
    "# for col in sharp_columns:\n",
    "#     sharp_df[col] -= sharp_df[col].mean()\n",
    "#     sharp_df[col] /= sharp_df[col].std()\n",
    "\n",
    "\n",
    "xray_df = pd.read_csv('data/xray.csv')\n",
    "xray_columns = [\"timestamp\", \"Short\", \"Long\"]\n",
    "xray_df = xray_df[xray_columns]\n",
    "xray_df = xray_df.dropna()\n",
    "\n",
    "# for col in xray_columns:\n",
    "#     xray_df[col] -= xray_df[col].mean()\n",
    "#     xray_df[col] /= xray_df[col].std()\n",
    "\n",
    "\n",
    "data = sharp_df.merge(xray_df, on='timestamp')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "\n",
    "def create_train_data_tuple():\n",
    "    random_time = datetime.fromtimestamp(int(data.sample(1)[\"timestamp\"]))\n",
    "    start, end = (random_time -\n",
    "                  timedelta(hours=1)).timestamp(), random_time.timestamp()\n",
    "    tmp_data = data[(data[\"timestamp\"] > start) & (data[\"timestamp\"] <= end)]\n",
    "    harps = tmp_data[\"harp\"].unique()\n",
    "\n",
    "    input_ = np.zeros(INPUT_DIM)\n",
    "    output = np.zeros(OUTPUT_DIM)\n",
    "\n",
    "    for i, harp in enumerate(harps):\n",
    "        if i == 5:\n",
    "            break\n",
    "        harp_data = tmp_data[tmp_data[\"harp\"] == harp]\n",
    "        input_data = harp_data[sharp_columns[1:]].to_numpy()\n",
    "        n_timesteps = input_data.shape[0]\n",
    "        if i == 0:\n",
    "            output_data = harp_data[[\"Short\", \"Long\"]].to_numpy()\n",
    "            output[:n_timesteps, :] = output_data\n",
    "\n",
    "        input_[:n_timesteps, i, :] = input_data\n",
    "\n",
    "    return input_, output\n",
    "\n",
    "\n",
    "def get_data(size=1000):\n",
    "    X, y = [], []\n",
    "    for _ in tqdm(range(size)):\n",
    "        X_, y_ = create_train_data_tuple()\n",
    "        X.append(X_)\n",
    "        y.append(y_)\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data(1000)\n",
    "X_val, y_val = get_data(100)\n",
    "\n",
    "print(np.any(np.isnan(X)))\n",
    "print(np.any(np.isnan(y)))\n",
    "print(np.any(np.isnan(X_val)))\n",
    "print(np.any(np.isnan(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    N = len(X) // BATCH_SIZE\n",
    "    for batch in tqdm(range(N)):\n",
    "        X_batch, y_batch = X[epoch*BATCH_SIZE:(epoch+1)*BATCH_SIZE], y[epoch*BATCH_SIZE:(epoch+1)*BATCH_SIZE]\n",
    "        X_batch, y_batch = np.array(X_batch), np.array(y_batch)\n",
    "        \n",
    "        assert not np.any(np.isnan(X_batch)), \"X_batch should not include nan\"\n",
    "        assert not np.any(np.isnan(y_batch)), \"y_batch should not include nan\"\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = t((X_batch, y_batch))\n",
    "        \n",
    "            assert not np.any(np.isnan(y_pred)), \"y_pred should not include nan\"\n",
    "        \n",
    "            loss_value = loss(y_batch, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(loss_value, t.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, t.trainable_variables))\n",
    "        \n",
    "        total_loss += loss_value.numpy()\n",
    "\n",
    "    print(f\"EPOCH {epoch}: mean_loss = {total_loss / N}\")\n",
    "    \n",
    "    y_pred_val, _ = t((np.array(X_val), np.array(y_val)))\n",
    "    val_loss = loss(y_val, y_pred_val).numpy()\n",
    "    \n",
    "    print(f\"EPOCH {epoch}: val_loss = {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abbc665463d35b62cca70309c790e7d0ad7bb1e4d5ea5962f40350218dd6908d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
