{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "import silence_tensorflow.auto\n",
    "import numpy as np\n",
    "\n",
    "N_TIMESTEPS = 5\n",
    "MAX_N_HARPS = 5\n",
    "N_FEATURES = 21\n",
    "INPUT_DIM = (N_TIMESTEPS, MAX_N_HARPS, N_FEATURES)\n",
    "N_OUT = 2\n",
    "OUTPUT_DIM =(N_TIMESTEPS,N_OUT)\n",
    "NUM_LAYERS = 3\n",
    "D_MODEL = 12\n",
    "DFF = 24\n",
    "NUM_HEADS = 3\n",
    "RATE = 0.1\n",
    "\n",
    "assert N_FEATURES > D_MODEL\n",
    "assert D_MODEL % NUM_HEADS == 0\n",
    "\n",
    "t = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_dimensions=INPUT_DIM,\n",
    "    target_dimensions=OUTPUT_DIM,\n",
    "    rate=RATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([np.random.random(INPUT_DIM) for _ in range(10)])\n",
    "y = np.array([np.random.random(OUTPUT_DIM) for _ in range(10)])\n",
    "\n",
    "y_pred, _ = t((X, y), training=True)\n",
    "np.any(np.isnan(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 570.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 621.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "metadata = {}\n",
    "sharp_df = pd.read_csv('data/sharp.csv')\n",
    "sharp_columns = [\n",
    "    \"timestamp\",\n",
    "    \"harp\",\n",
    "    \"USFLUX\",\n",
    "    \"MEANGAM\",\n",
    "    \"MEANGBT\",\n",
    "    \"MEANGBZ\",\n",
    "    \"MEANGBH\",\n",
    "    \"TOTPOT\",\n",
    "    \"TOTUSJZ\",\n",
    "    \"TOTUSJH\",\n",
    "    \"ABSNJZH\",\n",
    "    \"SAVNCPP\",\n",
    "    \"MEANPOT\",\n",
    "    \"MEANSHR\",\n",
    "    \"SHRGT45\",\n",
    "    \"SIZE\",\n",
    "    \"SIZE_ACR\",\n",
    "    \"NACR\",\n",
    "    \"NPIX\",\n",
    "    \"MEANJZD\",\n",
    "    \"MEANALP\",\n",
    "    \"MEANJZH\",\n",
    "]\n",
    "\n",
    "sharp_df = sharp_df[sharp_columns]\n",
    "sharp_df = sharp_df.dropna()\n",
    "sharp_df_notime = sharp_df[sharp_columns[1:]]\n",
    "\n",
    "metadata[\"sharp_mean\"] = sharp_df_notime.mean().to_numpy()\n",
    "metadata[\"sharp_std\"] = sharp_df_notime.std().to_numpy()\n",
    "sharp_df_notime -= sharp_df_notime.mean()\n",
    "sharp_df_notime /= sharp_df_notime.std()\n",
    "sharp_df_notime[\"timestamp\"] = sharp_df[\"timestamp\"]\n",
    "sharp_df = sharp_df_notime\n",
    "\n",
    "\n",
    "xray_df = pd.read_csv('data/xray.csv')\n",
    "xray_columns = [\"timestamp\", \"Short\", \"Long\"]\n",
    "xray_df = xray_df[xray_columns]\n",
    "xray_df = xray_df[(xray_df[\"Short\"] > 0.0) & (xray_df[\"Long\"] > 0.0)]\n",
    "xray_df_notime = xray_df[xray_columns[1:]]\n",
    "xray_df_notime = xray_df_notime.dropna()\n",
    "\n",
    "metadata[\"xray_mean\"] = xray_df_notime.mean().to_numpy()\n",
    "metadata[\"xray_std\"] = xray_df_notime.std().to_numpy()\n",
    "xray_df_notime -= xray_df_notime.mean()\n",
    "xray_df_notime += xray_df_notime.std()\n",
    "xray_df_notime[\"timestamp\"] = xray_df[\"timestamp\"]\n",
    "xray_df = xray_df_notime\n",
    "\n",
    "data = sharp_df.merge(xray_df, on='timestamp')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "\n",
    "def create_train_data_tuple():\n",
    "    random_time = datetime.fromtimestamp(int(data.sample(1)[\"timestamp\"]))\n",
    "    start, end = (random_time -\n",
    "                  timedelta(hours=1)).timestamp(), random_time.timestamp()\n",
    "    tmp_data = data[(data[\"timestamp\"] > start) & (data[\"timestamp\"] <= end)]\n",
    "    harps = tmp_data[\"harp\"].unique()\n",
    "\n",
    "    input_ = np.zeros(INPUT_DIM)\n",
    "    output = np.zeros(OUTPUT_DIM)\n",
    "\n",
    "    for i, harp in enumerate(harps):\n",
    "        if i == 5:\n",
    "            break\n",
    "        harp_data = tmp_data[tmp_data[\"harp\"] == harp]\n",
    "        input_data = harp_data[sharp_columns[1:]].to_numpy()\n",
    "        n_timesteps = input_data.shape[0]\n",
    "        if i == 0:\n",
    "            output_data = harp_data[[\"Short\", \"Long\"]].to_numpy()\n",
    "            output[:n_timesteps, :] = output_data\n",
    "\n",
    "        input_[:n_timesteps, i, :] = input_data\n",
    "\n",
    "    return input_, output\n",
    "\n",
    "\n",
    "def get_data(size=1000):\n",
    "    X, y = [], []\n",
    "    for _ in tqdm(range(size)):\n",
    "        X_, y_ = create_train_data_tuple()\n",
    "        X.append(X_)\n",
    "        y.append(y_)\n",
    "    return X, y\n",
    "\n",
    "X, y = get_data(1000)\n",
    "X_val, y_val = get_data(100)\n",
    "\n",
    "print(np.any(np.isnan(X)))\n",
    "print(np.any(np.isnan(y)))\n",
    "print(np.any(np.isnan(X_val)))\n",
    "print(np.any(np.isnan(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: mean_loss = 0.05319864000281086\n",
      "EPOCH 0: val_loss = 0.016247836872935295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: mean_loss = 0.0008050289155653445\n",
      "EPOCH 1: val_loss = 0.009096811525523663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: mean_loss = 0.0005437742564026848\n",
      "EPOCH 2: val_loss = 0.006016748026013374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: mean_loss = 0.0005651527261034062\n",
      "EPOCH 3: val_loss = 0.0025222860276699066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4: mean_loss = 0.0003520178583858069\n",
      "EPOCH 4: val_loss = 0.0035190414637327194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5: mean_loss = 0.00015316514294681836\n",
      "EPOCH 5: val_loss = 0.0024403678253293037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6: mean_loss = 0.00018476169545465383\n",
      "EPOCH 6: val_loss = 0.0022367103956639767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7: mean_loss = 0.00021666039833689865\n",
      "EPOCH 7: val_loss = 0.001420822343789041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8: mean_loss = 8.403554178585182e-05\n",
      "EPOCH 8: val_loss = 0.0015561613254249096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9: mean_loss = 2.5262686369842414e-05\n",
      "EPOCH 9: val_loss = 0.0010481922654435039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    N = len(X) // BATCH_SIZE\n",
    "    for batch in tqdm(range(N)):\n",
    "        X_batch, y_batch = X[epoch*BATCH_SIZE:(epoch+1)*BATCH_SIZE], y[epoch*BATCH_SIZE:(epoch+1)*BATCH_SIZE]\n",
    "        X_batch, y_batch = np.array(X_batch), np.array(y_batch)\n",
    "        \n",
    "        assert not np.any(np.isnan(X_batch)), \"X_batch should not include nan\"\n",
    "        assert not np.any(np.isnan(y_batch)), \"y_batch should not include nan\"\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = t((X_batch, y_batch), training=False)\n",
    "        \n",
    "            assert not np.any(np.isnan(y_pred)), \"y_pred should not include nan\"\n",
    "        \n",
    "            loss_value = loss(y_batch, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(loss_value, t.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, t.trainable_variables))\n",
    "        \n",
    "        total_loss += loss_value.numpy()\n",
    "\n",
    "    print(f\"EPOCH {epoch}: mean_loss = {total_loss / N}\")\n",
    "    \n",
    "    y_pred_val, _ = t((np.array(X_val), np.array(y_val)))\n",
    "    val_loss = loss(y_val, y_pred_val).numpy()\n",
    "    \n",
    "    print(f\"EPOCH {epoch}: val_loss = {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "MODELNAME = \"8h-1\"\n",
    "\n",
    "metadata[\"n_timesteps\"]  = N_TIMESTEPS\n",
    "metadata[\"max_n_harps\"]  = MAX_N_HARPS\n",
    "metadata[\"n_features\"] = N_FEATURES\n",
    "metadata[\"n_out\"]  = N_OUT\n",
    "metadata[\"num_layers\"] = NUM_LAYERS\n",
    "metadata[\"d_model\"] = D_MODEL\n",
    "metadata[\"num_heads\"] = NUM_HEADS\n",
    "metadata[\"dff\"] = DFF\n",
    "metadata[\"input_dim\"] = INPUT_DIM\n",
    "metadata[\"output_dim\"] = OUTPUT_DIM\n",
    "metadata[\"rate\"] = RATE\n",
    "\n",
    "\n",
    "\n",
    "t.save_weights(\"models/\"+MODELNAME)\n",
    "with open(\"meta/\"+MODELNAME+\".pkl\", \"wb\") as metafile:\n",
    "    metafile.write(pickle.dumps(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformer import Transformer\n",
    "\n",
    "\n",
    "class Avocato():\n",
    "    def __init__(self, modelname):\n",
    "        self.metadata = pickle.load(open(\"meta/\" + modelname+\".pkl\", \"rb\"))\n",
    "        self.model = Transformer(\n",
    "            num_layers = metadata[\"num_layers\"],\n",
    "            d_model = metadata[\"d_model\"],\n",
    "            num_heads = metadata[\"num_heads\"],\n",
    "            dff = metadata[\"dff\"],\n",
    "            input_dimensions = metadata[\"input_dim\"],\n",
    "            target_dimensions = metadata[\"output_dim\"],\n",
    "            rate = metadata[\"rate\"],\n",
    "        )\n",
    "        self.model.compile()\n",
    "        self.model.load_weights(\"models/\" + modelname)\n",
    "\n",
    "    def __call__(self, net_in):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "            - net_in: pd dataframe consisting of n_timesteps of data for HARPS (timestamp, harp, ...parameters)\n",
    "        \"\"\"\n",
    "\n",
    "        in_data = np.zeros(\n",
    "            (1, self.metadata[\"n_timesteps\"], self.metadata[\"max_n_harps\"],\n",
    "             self.metadata[\"n_features\"]))\n",
    "        unique_timesteps = net_in[\"timestamp\"].unique()\n",
    "        for t_id, timestep in enumerate(unique_timesteps):\n",
    "            if t_id >= self.metadata[\"n_timesteps\"]:\n",
    "                continue\n",
    "            unique_harps = net_in[net_in[\"timestamp\"] ==\n",
    "                                  timestep][\"harp\"].unique()\n",
    "            for h_id, harp in enumerate(unique_harps):\n",
    "                if h_id >= self.metadata[\"max_n_harps\"]:\n",
    "                    continue\n",
    "                in_data[0, t_id, h_id] = net_in[\n",
    "                    (net_in[\"timestamp\"] == timestep) &\n",
    "                    (net_in[\"harp\"] == harp\n",
    "                     )].loc[:,\n",
    "                            net_in.columns != \"timestamp\"].to_numpy().reshape(\n",
    "                                self.metadata[\"n_features\"])\n",
    "                in_data[0, t_id, h_id] -= self.metadata[\"sharp_mean\"]\n",
    "                in_data[0, t_id, h_id] /= self.metadata[\"sharp_std\"]\n",
    "\n",
    "        output = np.zeros((\n",
    "            1,\n",
    "            self.metadata[\"n_timesteps\"],\n",
    "            self.metadata[\"n_out\"],\n",
    "        ))\n",
    "\n",
    "        for i in tf.range(self.metadata[\"n_timesteps\"]):\n",
    "            output, _ = self.model([in_data, output],\n",
    "                                              training=False)\n",
    "\n",
    "        return output * self.metadata[\"xray_std\"] + self.metadata[\"xray_mean\"], output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Avocato(MODELNAME)\n",
    "random_time = datetime.fromtimestamp(int(data.sample(1)[\"timestamp\"]))\n",
    "start, end = (random_time -\n",
    "                timedelta(hours=1)).timestamp(), random_time.timestamp()\n",
    "net_in = data[(data[\"timestamp\"] > start) & (data[\"timestamp\"] <= end)]\n",
    "net_in = net_in[sharp_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 5, 2), dtype=float32, numpy=\n",
       " array([[[2.9338644e-08, 8.2975248e-08],\n",
       "         [2.9338644e-08, 8.2975248e-08],\n",
       "         [2.9338640e-08, 8.2975255e-08],\n",
       "         [2.9338642e-08, 8.2975248e-08],\n",
       "         [2.9338645e-08, 8.2975248e-08]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5, 2), dtype=float32, numpy=\n",
       " array([[[0.39534798, 0.02663079],\n",
       "         [0.39534798, 0.02663079],\n",
       "         [0.3953478 , 0.02663084],\n",
       "         [0.39534786, 0.02663079],\n",
       "         [0.3953481 , 0.02663079]]], dtype=float32)>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(net_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abbc665463d35b62cca70309c790e7d0ad7bb1e4d5ea5962f40350218dd6908d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
